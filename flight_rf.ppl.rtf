{\rtf1\ansi\ansicpg1252\deff0\nouicompat\deflang1033{\fonttbl{\f0\fnil\fcharset0 Calibri;}{\f1\fnil\fcharset1 Cambria Math;}}
{\*\generator Riched20 10.0.19041}{\*\mmathPr\mmathFont1\mwrapIndent1440 }\viewkind4\uc1 
\pard\sa200\sl276\slmult1\f0\fs22\lang9 import numpy as np\par
import pandas as pd\par
import matplotlib.pyplot as plt\par
import seaborn as sns\par
\par
sns.set()\par
train_data=pd.read_excel("C:\\\\Users\\\\lodhi\\\\Downloads\\\\Data_Train.xlsx")\par
pd.set_option('display.max_columns', None)\par
train_data.head()\par
Airline\tab Date_of_Journey\tab Source\tab Destination\tab Route\tab Dep_Time\tab Arrival_Time\tab Duration\tab Total_Stops\tab Additional_Info\tab Price\par
0\tab IndiGo\tab 24/03/2019\tab Banglore\tab New Delhi\tab BLR \f1\u8594?\f0  DEL\tab 22:20\tab 01:10 22 Mar\tab 2h 50m\tab non-stop\tab No info\tab 3897\par
1\tab Air India\tab 1/05/2019\tab Kolkata\tab Banglore\tab CCU \f1\u8594?\f0  IXR \f1\u8594?\f0  BBI \f1\u8594?\f0  BLR\tab 05:50\tab 13:15\tab 7h 25m\tab 2 stops\tab No info\tab 7662\par
2\tab Jet Airways\tab 9/06/2019\tab Delhi\tab Cochin\tab DEL \f1\u8594?\f0  LKO \f1\u8594?\f0  BOM \f1\u8594?\f0  COK\tab 09:25\tab 04:25 10 Jun\tab 19h\tab 2 stops\tab No info\tab 13882\par
3\tab IndiGo\tab 12/05/2019\tab Kolkata\tab Banglore\tab CCU \f1\u8594?\f0  NAG \f1\u8594?\f0  BLR\tab 18:05\tab 23:30\tab 5h 25m\tab 1 stop\tab No info\tab 6218\par
4\tab IndiGo\tab 01/03/2019\tab Banglore\tab New Delhi\tab BLR \f1\u8594?\f0  NAG \f1\u8594?\f0  DEL\tab 16:50\tab 21:35\tab 4h 45m\tab 1 stop\tab No info\tab 13302\par
train_data.info()\par
<class 'pandas.core.frame.DataFrame'>\par
RangeIndex: 10683 entries, 0 to 10682\par
Data columns (total 11 columns):\par
 #   Column           Non-Null Count  Dtype \par
---  ------           --------------  ----- \par
 0   Airline          10683 non-null  object\par
 1   Date_of_Journey  10683 non-null  object\par
 2   Source           10683 non-null  object\par
 3   Destination      10683 non-null  object\par
 4   Route            10682 non-null  object\par
 5   Dep_Time         10683 non-null  object\par
 6   Arrival_Time     10683 non-null  object\par
 7   Duration         10683 non-null  object\par
 8   Total_Stops      10682 non-null  object\par
 9   Additional_Info  10683 non-null  object\par
 10  Price            10683 non-null  int64 \par
dtypes: int64(1), object(10)\par
memory usage: 918.2+ KB\par
train_data["Duration"].value_counts()\par
2h 50m     550\par
1h 30m     386\par
2h 45m     337\par
2h 55m     337\par
2h 35m     329\par
          ... \par
31h 30m      1\par
30h 25m      1\par
42h 5m       1\par
4h 10m       1\par
47h 40m      1\par
Name: Duration, Length: 368, dtype: int64\par
train_data.dropna(inplace = True)\par
train_data.isnull().sum()\par
Airline            0\par
Date_of_Journey    0\par
Source             0\par
Destination        0\par
Route              0\par
Dep_Time           0\par
Arrival_Time       0\par
Duration           0\par
Total_Stops        0\par
Additional_Info    0\par
Price              0\par
dtype: int64\par
train_data["Journey_day"] = pd.to_datetime(train_data.Date_of_Journey, format="%d/%m/%Y").dt.day\par
train_data["Journey_month"] = pd.to_datetime(train_data["Date_of_Journey"], format = "%d/%m/%Y").dt.month\par
train_data.head()\par
Airline\tab Date_of_Journey\tab Source\tab Destination\tab Route\tab Dep_Time\tab Arrival_Time\tab Duration\tab Total_Stops\tab Additional_Info\tab Price\tab Journey_day\tab Journey_month\par
0\tab IndiGo\tab 24/03/2019\tab Banglore\tab New Delhi\tab BLR \f1\u8594?\f0  DEL\tab 22:20\tab 01:10 22 Mar\tab 2h 50m\tab non-stop\tab No info\tab 3897\tab 24\tab 3\par
1\tab Air India\tab 1/05/2019\tab Kolkata\tab Banglore\tab CCU \f1\u8594?\f0  IXR \f1\u8594?\f0  BBI \f1\u8594?\f0  BLR\tab 05:50\tab 13:15\tab 7h 25m\tab 2 stops\tab No info\tab 7662\tab 1\tab 5\par
2\tab Jet Airways\tab 9/06/2019\tab Delhi\tab Cochin\tab DEL \f1\u8594?\f0  LKO \f1\u8594?\f0  BOM \f1\u8594?\f0  COK\tab 09:25\tab 04:25 10 Jun\tab 19h\tab 2 stops\tab No info\tab 13882\tab 9\tab 6\par
3\tab IndiGo\tab 12/05/2019\tab Kolkata\tab Banglore\tab CCU \f1\u8594?\f0  NAG \f1\u8594?\f0  BLR\tab 18:05\tab 23:30\tab 5h 25m\tab 1 stop\tab No info\tab 6218\tab 12\tab 5\par
4\tab IndiGo\tab 01/03/2019\tab Banglore\tab New Delhi\tab BLR \f1\u8594?\f0  NAG \f1\u8594?\f0  DEL\tab 16:50\tab 21:35\tab 4h 45m\tab 1 stop\tab No info\tab 13302\tab 1\tab 3\par
# Since we have converted Date_of_Journey column into integers, Now we can drop as it is of no use.\par
\par
train_data.drop(["Date_of_Journey"], axis = 1, inplace = True)\par
# Departure time is when a plane leaves the gate. \par
# Similar to Date_of_Journey we can extract values from Dep_Time\par
\par
# Extracting Hours\par
train_data["Dep_hour"] = pd.to_datetime(train_data["Dep_Time"]).dt.hour\par
\par
# Extracting Minutes\par
train_data["Dep_min"] = pd.to_datetime(train_data["Dep_Time"]).dt.minute\par
\par
# Now we can drop Dep_Time as it is of no use\par
train_data.drop(["Dep_Time"], axis = 1, inplace = True)\par
train_data.head()\par
Airline\tab Source\tab Destination\tab Route\tab Arrival_Time\tab Duration\tab Total_Stops\tab Additional_Info\tab Price\tab Journey_day\tab Journey_month\tab Dep_hour\tab Dep_min\par
0\tab IndiGo\tab Banglore\tab New Delhi\tab BLR \f1\u8594?\f0  DEL\tab 01:10 22 Mar\tab 2h 50m\tab non-stop\tab No info\tab 3897\tab 24\tab 3\tab 22\tab 20\par
1\tab Air India\tab Kolkata\tab Banglore\tab CCU \f1\u8594?\f0  IXR \f1\u8594?\f0  BBI \f1\u8594?\f0  BLR\tab 13:15\tab 7h 25m\tab 2 stops\tab No info\tab 7662\tab 1\tab 5\tab 5\tab 50\par
2\tab Jet Airways\tab Delhi\tab Cochin\tab DEL \f1\u8594?\f0  LKO \f1\u8594?\f0  BOM \f1\u8594?\f0  COK\tab 04:25 10 Jun\tab 19h\tab 2 stops\tab No info\tab 13882\tab 9\tab 6\tab 9\tab 25\par
3\tab IndiGo\tab Kolkata\tab Banglore\tab CCU \f1\u8594?\f0  NAG \f1\u8594?\f0  BLR\tab 23:30\tab 5h 25m\tab 1 stop\tab No info\tab 6218\tab 12\tab 5\tab 18\tab 5\par
4\tab IndiGo\tab Banglore\tab New Delhi\tab BLR \f1\u8594?\f0  NAG \f1\u8594?\f0  DEL\tab 21:35\tab 4h 45m\tab 1 stop\tab No info\tab 13302\tab 1\tab 3\tab 16\tab 50\par
# Arrival time is when the plane pulls up to the gate.\par
# Similar to Date_of_Journey we can extract values from Arrival_Time\par
\par
# Extracting Hours\par
train_data["Arrival_hour"] = pd.to_datetime(train_data.Arrival_Time).dt.hour\par
\par
# Extracting Minutes\par
train_data["Arrival_min"] = pd.to_datetime(train_data.Arrival_Time).dt.minute\par
\par
# Now we can drop Arrival_Time as it is of no use\par
train_data.drop(["Arrival_Time"], axis = 1, inplace = True)\par
train_data.head()\par
Airline\tab Source\tab Destination\tab Route\tab Duration\tab Total_Stops\tab Additional_Info\tab Price\tab Journey_day\tab Journey_month\tab Dep_hour\tab Dep_min\tab Arrival_hour\tab Arrival_min\par
0\tab IndiGo\tab Banglore\tab New Delhi\tab BLR \f1\u8594?\f0  DEL\tab 2h 50m\tab non-stop\tab No info\tab 3897\tab 24\tab 3\tab 22\tab 20\tab 1\tab 10\par
1\tab Air India\tab Kolkata\tab Banglore\tab CCU \f1\u8594?\f0  IXR \f1\u8594?\f0  BBI \f1\u8594?\f0  BLR\tab 7h 25m\tab 2 stops\tab No info\tab 7662\tab 1\tab 5\tab 5\tab 50\tab 13\tab 15\par
2\tab Jet Airways\tab Delhi\tab Cochin\tab DEL \f1\u8594?\f0  LKO \f1\u8594?\f0  BOM \f1\u8594?\f0  COK\tab 19h\tab 2 stops\tab No info\tab 13882\tab 9\tab 6\tab 9\tab 25\tab 4\tab 25\par
3\tab IndiGo\tab Kolkata\tab Banglore\tab CCU \f1\u8594?\f0  NAG \f1\u8594?\f0  BLR\tab 5h 25m\tab 1 stop\tab No info\tab 6218\tab 12\tab 5\tab 18\tab 5\tab 23\tab 30\par
4\tab IndiGo\tab Banglore\tab New Delhi\tab BLR \f1\u8594?\f0  NAG \f1\u8594?\f0  DEL\tab 4h 45m\tab 1 stop\tab No info\tab 13302\tab 1\tab 3\tab 16\tab 50\tab 21\tab 35\par
# Time taken by plane to reach destination is called Duration\par
# It is the differnce betwwen Departure Time and Arrival time\par
\par
\par
# Assigning and converting Duration column into list\par
duration = list(train_data["Duration"])\par
\par
for i in range(len(duration)):\par
    if len(duration[i].split()) != 2:    # Check if duration contains only hour or mins\par
        if "h" in duration[i]:\par
            duration[i] = duration[i].strip() + " 0m"   # Adds 0 minute\par
        else:\par
            duration[i] = "0h " + duration[i]           # Adds 0 hour\par
\par
duration_hours = []\par
duration_mins = []\par
for i in range(len(duration)):\par
    duration_hours.append(int(duration[i].split(sep = "h")[0]))    # Extract hours from duration\par
    duration_mins.append(int(duration[i].split(sep = "m")[0].split()[-1]))   # Extracts only minutes from duration\par
# Adding duration_hours and duration_mins list to train_data dataframe\par
\par
train_data["Duration_hours"] = duration_hours\par
train_data["Duration_mins"] = duration_mins\par
train_data.drop(["Duration"], axis = 1, inplace = True)\par
train_data.head()\par
Airline\tab Source\tab Destination\tab Route\tab Total_Stops\tab Additional_Info\tab Price\tab Journey_day\tab Journey_month\tab Dep_hour\tab Dep_min\tab Arrival_hour\tab Arrival_min\tab Duration_hours\tab Duration_mins\par
0\tab IndiGo\tab Banglore\tab New Delhi\tab BLR \f1\u8594?\f0  DEL\tab non-stop\tab No info\tab 3897\tab 24\tab 3\tab 22\tab 20\tab 1\tab 10\tab 2\tab 50\par
1\tab Air India\tab Kolkata\tab Banglore\tab CCU \f1\u8594?\f0  IXR \f1\u8594?\f0  BBI \f1\u8594?\f0  BLR\tab 2 stops\tab No info\tab 7662\tab 1\tab 5\tab 5\tab 50\tab 13\tab 15\tab 7\tab 25\par
2\tab Jet Airways\tab Delhi\tab Cochin\tab DEL \f1\u8594?\f0  LKO \f1\u8594?\f0  BOM \f1\u8594?\f0  COK\tab 2 stops\tab No info\tab 13882\tab 9\tab 6\tab 9\tab 25\tab 4\tab 25\tab 19\tab 0\par
3\tab IndiGo\tab Kolkata\tab Banglore\tab CCU \f1\u8594?\f0  NAG \f1\u8594?\f0  BLR\tab 1 stop\tab No info\tab 6218\tab 12\tab 5\tab 18\tab 5\tab 23\tab 30\tab 5\tab 25\par
4\tab IndiGo\tab Banglore\tab New Delhi\tab BLR \f1\u8594?\f0  NAG \f1\u8594?\f0  DEL\tab 1 stop\tab No info\tab 13302\tab 1\tab 3\tab 16\tab 50\tab 21\tab 35\tab 4\tab 45\par
# handling categorical variable\par
train_data["Airline"].value_counts()\par
Jet Airways                          3849\par
IndiGo                               2053\par
Air India                            1751\par
Multiple carriers                    1196\par
SpiceJet                              818\par
Vistara                               479\par
Air Asia                              319\par
GoAir                                 194\par
Multiple carriers Premium economy      13\par
Jet Airways Business                    6\par
Vistara Premium economy                 3\par
Trujet                                  1\par
Name: Airline, dtype: int64\par
# From graph we can see that Jet Airways Business have the highest Price.\par
# Apart from the first Airline almost all are having similar median\par
\par
# Airline vs Price\par
sns.catplot(y = "Price", x = "Airline", data = train_data.sort_values("Price", ascending = False), kind="boxen", height = 6, aspect = 3)\par
plt.show()\par
\par
# As Airline is Nominal Categorical data we will perform OneHotEncoding\par
\par
Airline = train_data[["Airline"]]\par
\par
Airline = pd.get_dummies(Airline, drop_first= True)\par
\par
Airline.head()\par
Airline_Air India\tab Airline_GoAir\tab Airline_IndiGo\tab Airline_Jet Airways\tab Airline_Jet Airways Business\tab Airline_Multiple carriers\tab Airline_Multiple carriers Premium economy\tab Airline_SpiceJet\tab Airline_Trujet\tab Airline_Vistara\tab Airline_Vistara Premium economy\par
0\tab 0\tab 0\tab 1\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\par
1\tab 1\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\par
2\tab 0\tab 0\tab 0\tab 1\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\par
3\tab 0\tab 0\tab 1\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\par
4\tab 0\tab 0\tab 1\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\par
train_data["Source"].value_counts()\par
Delhi       4536\par
Kolkata     2871\par
Banglore    2197\par
Mumbai       697\par
Chennai      381\par
Name: Source, dtype: int64\par
# Source vs Price\par
\par
sns.catplot(y = "Price", x = "Source", data = train_data.sort_values("Price", ascending = False), kind="boxen", height = 4, aspect = 3)\par
plt.show()\par
\par
# As Source is Nominal Categorical data we will perform OneHotEncoding\par
\par
Source = train_data[["Source"]]\par
\par
Source = pd.get_dummies(Source, drop_first= True)\par
\par
Source.head()\par
Source_Chennai\tab Source_Delhi\tab Source_Kolkata\tab Source_Mumbai\par
0\tab 0\tab 0\tab 0\tab 0\par
1\tab 0\tab 0\tab 1\tab 0\par
2\tab 0\tab 1\tab 0\tab 0\par
3\tab 0\tab 0\tab 1\tab 0\par
4\tab 0\tab 0\tab 0\tab 0\par
train_data["Destination"].value_counts()\par
Cochin       4536\par
Banglore     2871\par
Delhi        1265\par
New Delhi     932\par
Hyderabad     697\par
Kolkata       381\par
Name: Destination, dtype: int64\par
# As Destination is Nominal Categorical data we will perform OneHotEncoding\par
\par
Destination = train_data[["Destination"]]\par
\par
Destination = pd.get_dummies(Destination, drop_first = True)\par
\par
Destination.head()\par
Destination_Cochin\tab Destination_Delhi\tab Destination_Hyderabad\tab Destination_Kolkata\tab Destination_New Delhi\par
0\tab 0\tab 0\tab 0\tab 0\tab 1\par
1\tab 0\tab 0\tab 0\tab 0\tab 0\par
2\tab 1\tab 0\tab 0\tab 0\tab 0\par
3\tab 0\tab 0\tab 0\tab 0\tab 0\par
4\tab 0\tab 0\tab 0\tab 0\tab 1\par
train_data["Route"]\par
0                    BLR \f1\u8594?\f0  DEL\par
1        CCU \f1\u8594?\f0  IXR \f1\u8594?\f0  BBI \f1\u8594?\f0  BLR\par
2        DEL \f1\u8594?\f0  LKO \f1\u8594?\f0  BOM \f1\u8594?\f0  COK\par
3              CCU \f1\u8594?\f0  NAG \f1\u8594?\f0  BLR\par
4              BLR \f1\u8594?\f0  NAG \f1\u8594?\f0  DEL\par
                 ...          \par
10678                CCU \f1\u8594?\f0  BLR\par
10679                CCU \f1\u8594?\f0  BLR\par
10680                BLR \f1\u8594?\f0  DEL\par
10681                BLR \f1\u8594?\f0  DEL\par
10682    DEL \f1\u8594?\f0  GOI \f1\u8594?\f0  BOM \f1\u8594?\f0  COK\par
Name: Route, Length: 10682, dtype: object\par
# Additional_Info contains almost 80% no_info\par
# Route and Total_Stops are related to each other\par
\par
train_data.drop(["Route", "Additional_Info"], axis = 1, inplace = True)\par
train_data["Total_Stops"].value_counts()\par
1 stop      5625\par
non-stop    3491\par
2 stops     1520\par
3 stops       45\par
4 stops        1\par
Name: Total_Stops, dtype: int64\par
# As this is case of Ordinal Categorical type we perform LabelEncoder\par
# Here Values are assigned with corresponding keys\par
\par
train_data.replace(\{"non-stop": 0, "1 stop": 1, "2 stops": 2, "3 stops": 3, "4 stops": 4\}, inplace = True)\par
train_data.head()\par
Airline\tab Source\tab Destination\tab Total_Stops\tab Price\tab Journey_day\tab Journey_month\tab Dep_hour\tab Dep_min\tab Arrival_hour\tab Arrival_min\tab Duration_hours\tab Duration_mins\par
0\tab IndiGo\tab Banglore\tab New Delhi\tab 0\tab 3897\tab 24\tab 3\tab 22\tab 20\tab 1\tab 10\tab 2\tab 50\par
1\tab Air India\tab Kolkata\tab Banglore\tab 2\tab 7662\tab 1\tab 5\tab 5\tab 50\tab 13\tab 15\tab 7\tab 25\par
2\tab Jet Airways\tab Delhi\tab Cochin\tab 2\tab 13882\tab 9\tab 6\tab 9\tab 25\tab 4\tab 25\tab 19\tab 0\par
3\tab IndiGo\tab Kolkata\tab Banglore\tab 1\tab 6218\tab 12\tab 5\tab 18\tab 5\tab 23\tab 30\tab 5\tab 25\par
4\tab IndiGo\tab Banglore\tab New Delhi\tab 1\tab 13302\tab 1\tab 3\tab 16\tab 50\tab 21\tab 35\tab 4\tab 45\par
# Concatenate dataframe --> train_data + Airline + Source + Destination\par
\par
data_train = pd.concat([train_data, Airline, Source, Destination], axis = 1)\par
data_train.head()\par
Airline\tab Source\tab Destination\tab Total_Stops\tab Price\tab Journey_day\tab Journey_month\tab Dep_hour\tab Dep_min\tab Arrival_hour\tab Arrival_min\tab Duration_hours\tab Duration_mins\tab Airline_Air India\tab Airline_GoAir\tab Airline_IndiGo\tab Airline_Jet Airways\tab Airline_Jet Airways Business\tab Airline_Multiple carriers\tab Airline_Multiple carriers Premium economy\tab Airline_SpiceJet\tab Airline_Trujet\tab Airline_Vistara\tab Airline_Vistara Premium economy\tab Source_Chennai\tab Source_Delhi\tab Source_Kolkata\tab Source_Mumbai\tab Destination_Cochin\tab Destination_Delhi\tab Destination_Hyderabad\tab Destination_Kolkata\tab Destination_New Delhi\par
0\tab IndiGo\tab Banglore\tab New Delhi\tab 0\tab 3897\tab 24\tab 3\tab 22\tab 20\tab 1\tab 10\tab 2\tab 50\tab 0\tab 0\tab 1\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 1\par
1\tab Air India\tab Kolkata\tab Banglore\tab 2\tab 7662\tab 1\tab 5\tab 5\tab 50\tab 13\tab 15\tab 7\tab 25\tab 1\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 1\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\par
2\tab Jet Airways\tab Delhi\tab Cochin\tab 2\tab 13882\tab 9\tab 6\tab 9\tab 25\tab 4\tab 25\tab 19\tab 0\tab 0\tab 0\tab 0\tab 1\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 1\tab 0\tab 0\tab 1\tab 0\tab 0\tab 0\tab 0\par
3\tab IndiGo\tab Kolkata\tab Banglore\tab 1\tab 6218\tab 12\tab 5\tab 18\tab 5\tab 23\tab 30\tab 5\tab 25\tab 0\tab 0\tab 1\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 1\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\par
4\tab IndiGo\tab Banglore\tab New Delhi\tab 1\tab 13302\tab 1\tab 3\tab 16\tab 50\tab 21\tab 35\tab 4\tab 45\tab 0\tab 0\tab 1\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 1\par
data_train.drop(["Airline", "Source", "Destination"], axis = 1, inplace = True)\par
data_train.head()\par
Total_Stops\tab Price\tab Journey_day\tab Journey_month\tab Dep_hour\tab Dep_min\tab Arrival_hour\tab Arrival_min\tab Duration_hours\tab Duration_mins\tab Airline_Air India\tab Airline_GoAir\tab Airline_IndiGo\tab Airline_Jet Airways\tab Airline_Jet Airways Business\tab Airline_Multiple carriers\tab Airline_Multiple carriers Premium economy\tab Airline_SpiceJet\tab Airline_Trujet\tab Airline_Vistara\tab Airline_Vistara Premium economy\tab Source_Chennai\tab Source_Delhi\tab Source_Kolkata\tab Source_Mumbai\tab Destination_Cochin\tab Destination_Delhi\tab Destination_Hyderabad\tab Destination_Kolkata\tab Destination_New Delhi\par
0\tab 0\tab 3897\tab 24\tab 3\tab 22\tab 20\tab 1\tab 10\tab 2\tab 50\tab 0\tab 0\tab 1\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 1\par
1\tab 2\tab 7662\tab 1\tab 5\tab 5\tab 50\tab 13\tab 15\tab 7\tab 25\tab 1\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 1\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\par
2\tab 2\tab 13882\tab 9\tab 6\tab 9\tab 25\tab 4\tab 25\tab 19\tab 0\tab 0\tab 0\tab 0\tab 1\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 1\tab 0\tab 0\tab 1\tab 0\tab 0\tab 0\tab 0\par
3\tab 1\tab 6218\tab 12\tab 5\tab 18\tab 5\tab 23\tab 30\tab 5\tab 25\tab 0\tab 0\tab 1\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 1\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\par
4\tab 1\tab 13302\tab 1\tab 3\tab 16\tab 50\tab 21\tab 35\tab 4\tab 45\tab 0\tab 0\tab 1\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 1\par
data_train.shape\par
(10682, 30)\par
# test data\par
test_data=pd.read_excel("C:\\\\Users\\\\lodhi\\\\Downloads\\\\Test_set.xlsx")\par
test_data.head()\par
Airline\tab Date_of_Journey\tab Source\tab Destination\tab Route\tab Dep_Time\tab Arrival_Time\tab Duration\tab Total_Stops\tab Additional_Info\par
0\tab Jet Airways\tab 6/06/2019\tab Delhi\tab Cochin\tab DEL \f1\u8594?\f0  BOM \f1\u8594?\f0  COK\tab 17:30\tab 04:25 07 Jun\tab 10h 55m\tab 1 stop\tab No info\par
1\tab IndiGo\tab 12/05/2019\tab Kolkata\tab Banglore\tab CCU \f1\u8594?\f0  MAA \f1\u8594?\f0  BLR\tab 06:20\tab 10:20\tab 4h\tab 1 stop\tab No info\par
2\tab Jet Airways\tab 21/05/2019\tab Delhi\tab Cochin\tab DEL \f1\u8594?\f0  BOM \f1\u8594?\f0  COK\tab 19:15\tab 19:00 22 May\tab 23h 45m\tab 1 stop\tab In-flight meal not included\par
3\tab Multiple carriers\tab 21/05/2019\tab Delhi\tab Cochin\tab DEL \f1\u8594?\f0  BOM \f1\u8594?\f0  COK\tab 08:00\tab 21:00\tab 13h\tab 1 stop\tab No info\par
4\tab Air Asia\tab 24/06/2019\tab Banglore\tab Delhi\tab BLR \f1\u8594?\f0  DEL\tab 23:55\tab 02:45 25 Jun\tab 2h 50m\tab non-stop\tab No info\par
# Preprocessing\par
\par
print("Test data Info")\par
print("-"*75)\par
print(test_data.info())\par
\par
print()\par
print()\par
\par
print("Null values :")\par
print("-"*75)\par
test_data.dropna(inplace = True)\par
print(test_data.isnull().sum())\par
\par
# EDA\par
\par
# Date_of_Journey\par
test_data["Journey_day"] = pd.to_datetime(test_data.Date_of_Journey, format="%d/%m/%Y").dt.day\par
test_data["Journey_month"] = pd.to_datetime(test_data["Date_of_Journey"], format = "%d/%m/%Y").dt.month\par
test_data.drop(["Date_of_Journey"], axis = 1, inplace = True)\par
\par
# Dep_Time\par
test_data["Dep_hour"] = pd.to_datetime(test_data["Dep_Time"]).dt.hour\par
test_data["Dep_min"] = pd.to_datetime(test_data["Dep_Time"]).dt.minute\par
test_data.drop(["Dep_Time"], axis = 1, inplace = True)\par
\par
# Arrival_Time\par
test_data["Arrival_hour"] = pd.to_datetime(test_data.Arrival_Time).dt.hour\par
test_data["Arrival_min"] = pd.to_datetime(test_data.Arrival_Time).dt.minute\par
test_data.drop(["Arrival_Time"], axis = 1, inplace = True)\par
# Duration\par
duration = list(test_data["Duration"])\par
\par
for i in range(len(duration)):\par
    if len(duration[i].split()) != 2:    # Check if duration contains only hour or mins\par
        if "h" in duration[i]:\par
            duration[i] = duration[i].strip() + " 0m"   # Adds 0 minute\par
        else:\par
            duration[i] = "0h " + duration[i]           # Adds 0 hour\par
\par
duration_hours = []\par
duration_mins = []\par
for i in range(len(duration)):\par
    duration_hours.append(int(duration[i].split(sep = "h")[0]))    # Extract hours from duration\par
    duration_mins.append(int(duration[i].split(sep = "m")[0].split()[-1]))   # Extracts only minutes from duration\par
# Adding Duration column to test set\par
test_data["Duration_hours"] = duration_hours\par
test_data["Duration_mins"] = duration_mins\par
test_data.drop(["Duration"], axis = 1, inplace = True)\par
# Categorical data\par
\par
print("Airline")\par
print("-"*75)\par
print(test_data["Airline"].value_counts())\par
Airline = pd.get_dummies(test_data["Airline"], drop_first= True)\par
\par
print()\par
\par
print("Source")\par
print("-"*75)\par
print(test_data["Source"].value_counts())\par
Source = pd.get_dummies(test_data["Source"], drop_first= True)\par
\par
print()\par
\par
print("Destination")\par
print("-"*75)\par
print(test_data["Destination"].value_counts())\par
Destination = pd.get_dummies(test_data["Destination"], drop_first = True)\par
Airline\par
---------------------------------------------------------------------------\par
Jet Airways                          897\par
IndiGo                               511\par
Air India                            440\par
Multiple carriers                    347\par
SpiceJet                             208\par
Vistara                              129\par
Air Asia                              86\par
GoAir                                 46\par
Multiple carriers Premium economy      3\par
Vistara Premium economy                2\par
Jet Airways Business                   2\par
Name: Airline, dtype: int64\par
\par
Source\par
---------------------------------------------------------------------------\par
Delhi       1145\par
Kolkata      710\par
Banglore     555\par
Mumbai       186\par
Chennai       75\par
Name: Source, dtype: int64\par
\par
Destination\par
---------------------------------------------------------------------------\par
Cochin       1145\par
Banglore      710\par
Delhi         317\par
New Delhi     238\par
Hyderabad     186\par
Kolkata        75\par
Name: Destination, dtype: int64\par
# Additional_Info contains almost 80% no_info\par
# Route and Total_Stops are related to each other\par
test_data.drop(["Route", "Additional_Info"], axis = 1, inplace = True)\par
\par
# Replacing Total_Stops\par
test_data.replace(\{"non-stop": 0, "1 stop": 1, "2 stops": 2, "3 stops": 3, "4 stops": 4\}, inplace = True)\par
\par
# Concatenate dataframe --> test_data + Airline + Source + Destination\par
data_test = pd.concat([test_data, Airline, Source, Destination], axis = 1)\par
\par
data_test.drop(["Airline", "Source", "Destination"], axis = 1, inplace = True)\par
\par
print()\par
print()\par
\par
print("Shape of test data : ", data_test.shape)\par
\par
Shape of test data :  (2671, 25)\par
data_test.head()\par
Date_of_Journey\tab Dep_Time\tab Arrival_Time\tab Total_Stops\tab Duration_hours\tab Duration_mins\tab Air India\tab GoAir\tab IndiGo\tab Jet Airways\tab Jet Airways Business\tab Multiple carriers\tab Multiple carriers Premium economy\tab SpiceJet\tab Vistara\tab Vistara Premium economy\tab Chennai\tab Delhi\tab Kolkata\tab Mumbai\tab Cochin\tab Delhi\tab Hyderabad\tab Kolkata\tab New Delhi\par
0\tab 6/06/2019\tab 17:30\tab 04:25 07 Jun\tab 1\tab 10\tab 55\tab 0\tab 0\tab 0\tab 1\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 1\tab 0\tab 0\tab 1\tab 0\tab 0\tab 0\tab 0\par
1\tab 12/05/2019\tab 06:20\tab 10:20\tab 1\tab 4\tab 0\tab 0\tab 0\tab 1\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 1\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\par
2\tab 21/05/2019\tab 19:15\tab 19:00 22 May\tab 1\tab 23\tab 45\tab 0\tab 0\tab 0\tab 1\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 1\tab 0\tab 0\tab 1\tab 0\tab 0\tab 0\tab 0\par
3\tab 21/05/2019\tab 08:00\tab 21:00\tab 1\tab 13\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 1\tab 0\tab 0\tab 0\tab 0\tab 0\tab 1\tab 0\tab 0\tab 1\tab 0\tab 0\tab 0\tab 0\par
4\tab 24/06/2019\tab 23:55\tab 02:45 25 Jun\tab 0\tab 2\tab 50\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 1\tab 0\tab 0\tab 0\par
# Feature Selection\par
'''Finding out the best feature which will contribute and have good relation with target variable. Following are some of the feature selection methods,\par
\par
heatmap\par
feature_importance_\par
SelectKBest'''\par
'Finding out the best feature which will contribute and have good relation with target variable. Following are some of the feature selection methods,\\n\\nheatmap\\nfeature_importance_\\nSelectKBest'\par
data_train.shape\par
(10682, 30)\par
data_train.columns\par
Index(['Total_Stops', 'Price', 'Journey_day', 'Journey_month', 'Dep_hour',\par
       'Dep_min', 'Arrival_hour', 'Arrival_min', 'Duration_hours',\par
       'Duration_mins', 'Airline_Air India', 'Airline_GoAir', 'Airline_IndiGo',\par
       'Airline_Jet Airways', 'Airline_Jet Airways Business',\par
       'Airline_Multiple carriers',\par
       'Airline_Multiple carriers Premium economy', 'Airline_SpiceJet',\par
       'Airline_Trujet', 'Airline_Vistara', 'Airline_Vistara Premium economy',\par
       'Source_Chennai', 'Source_Delhi', 'Source_Kolkata', 'Source_Mumbai',\par
       'Destination_Cochin', 'Destination_Delhi', 'Destination_Hyderabad',\par
       'Destination_Kolkata', 'Destination_New Delhi'],\par
      dtype='object')\par
X = data_train.loc[:, ['Total_Stops', 'Journey_day', 'Journey_month', 'Dep_hour',\par
       'Dep_min', 'Arrival_hour', 'Arrival_min', 'Duration_hours',\par
       'Duration_mins', 'Airline_Air India', 'Airline_GoAir', 'Airline_IndiGo',\par
       'Airline_Jet Airways', 'Airline_Jet Airways Business',\par
       'Airline_Multiple carriers',\par
       'Airline_Multiple carriers Premium economy', 'Airline_SpiceJet',\par
       'Airline_Trujet', 'Airline_Vistara', 'Airline_Vistara Premium economy',\par
       'Source_Chennai', 'Source_Delhi', 'Source_Kolkata', 'Source_Mumbai',\par
       'Destination_Cochin', 'Destination_Delhi', 'Destination_Hyderabad',\par
       'Destination_Kolkata', 'Destination_New Delhi']]\par
X.head()\par
Total_Stops\tab Journey_day\tab Journey_month\tab Dep_hour\tab Dep_min\tab Arrival_hour\tab Arrival_min\tab Duration_hours\tab Duration_mins\tab Airline_Air India\tab Airline_GoAir\tab Airline_IndiGo\tab Airline_Jet Airways\tab Airline_Jet Airways Business\tab Airline_Multiple carriers\tab Airline_Multiple carriers Premium economy\tab Airline_SpiceJet\tab Airline_Trujet\tab Airline_Vistara\tab Airline_Vistara Premium economy\tab Source_Chennai\tab Source_Delhi\tab Source_Kolkata\tab Source_Mumbai\tab Destination_Cochin\tab Destination_Delhi\tab Destination_Hyderabad\tab Destination_Kolkata\tab Destination_New Delhi\par
0\tab 0\tab 24\tab 3\tab 22\tab 20\tab 1\tab 10\tab 2\tab 50\tab 0\tab 0\tab 1\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 1\par
1\tab 2\tab 1\tab 5\tab 5\tab 50\tab 13\tab 15\tab 7\tab 25\tab 1\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 1\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\par
2\tab 2\tab 9\tab 6\tab 9\tab 25\tab 4\tab 25\tab 19\tab 0\tab 0\tab 0\tab 0\tab 1\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 1\tab 0\tab 0\tab 1\tab 0\tab 0\tab 0\tab 0\par
3\tab 1\tab 12\tab 5\tab 18\tab 5\tab 23\tab 30\tab 5\tab 25\tab 0\tab 0\tab 1\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 1\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\par
4\tab 1\tab 1\tab 3\tab 16\tab 50\tab 21\tab 35\tab 4\tab 45\tab 0\tab 0\tab 1\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 0\tab 1\par
y = data_train.iloc[:, 1]\par
y.head()\par
0     3897\par
1     7662\par
2    13882\par
3     6218\par
4    13302\par
Name: Price, dtype: int64\par
# Finds correlation between Independent and dependent attributes\par
\par
plt.figure(figsize = (18,18))\par
sns.heatmap(train_data.corr(), annot = True, cmap = "RdYlGn")\par
\par
plt.show()\par
\par
# Important feature using ExtraTreesRegressor\par
\par
from sklearn.ensemble import ExtraTreesRegressor\par
selection = ExtraTreesRegressor()\par
selection.fit(X, y)\par
\par
ExtraTreesRegressor\par
ExtraTreesRegressor()\par
print(selection.feature_importances_)\par
[2.27437870e-01 1.43693922e-01 5.38088962e-02 2.42728605e-02\par
 2.17022982e-02 2.74427441e-02 1.90277613e-02 1.23780413e-01\par
 1.76340636e-02 9.89033554e-03 1.82937766e-03 1.69429430e-02\par
 1.41309823e-01 6.73124769e-02 2.17579882e-02 8.58644485e-04\par
 3.12270342e-03 9.27250606e-05 5.09323042e-03 7.84249729e-05\par
 5.82311797e-04 8.31779974e-03 3.18661118e-03 6.26425868e-03\par
 7.61620529e-03 1.45016099e-02 7.09617201e-03 4.59945604e-04\par
 2.48855841e-02]\par
#plot graph of feature importances for better visualization\par
\par
plt.figure(figsize = (12,8))\par
feat_importances = pd.Series(selection.feature_importances_, index=X.columns)\par
feat_importances.nlargest(20).plot(kind='barh')\par
plt.show()\par
\par
#Fitting model using Random Forest\par
from sklearn.model_selection import train_test_split\par
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\par
from sklearn.ensemble import RandomForestRegressor\par
reg_rf = RandomForestRegressor()\par
reg_rf.fit(X_train, y_train)\par
\par
RandomForestRegressor\par
RandomForestRegressor()\par
y_pred = reg_rf.predict(X_test)\par
reg_rf.score(X_train, y_train)\par
0.9532577218613149\par
reg_rf.score(X_test, y_test)\par
0.7976686846835099\par
sns.distplot(y_test-y_pred)\par
plt.show()\par
C:\\Users\\lodhi\\anaconda3\\lib\\site-packages\\seaborn\\distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\par
  warnings.warn(msg, FutureWarning)\par
\par
plt.scatter(y_test, y_pred, alpha = 0.5)\par
plt.xlabel("y_test")\par
plt.ylabel("y_pred")\par
plt.show()\par
\par
from sklearn import metrics\par
print('MAE:', metrics.mean_absolute_error(y_test, y_pred))\par
print('MSE:', metrics.mean_squared_error(y_test, y_pred))\par
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\par
MAE: 1175.9930317534595\par
MSE: 4362679.216995898\par
RMSE: 2088.702759369053\par
# RMSE/(max(DV)-min(DV))\par
\par
2090.5509/(max(y)-min(y))\par
0.026887077025966846\par
metrics.r2_score(y_test, y_pred)\par
0.7976686846835099\par
# Hyperparameter Tuning\par
from sklearn.model_selection import RandomizedSearchCV\par
#Randomized Search CV\par
\par
# Number of trees in random forest\par
n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\par
# Number of features to consider at every split\par
max_features = ['auto', 'sqrt']\par
# Maximum number of levels in tree\par
max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\par
# Minimum number of samples required to split a node\par
min_samples_split = [2, 5, 10, 15, 100]\par
# Minimum number of samples required at each leaf node\par
min_samples_leaf = [1, 2, 5, 10]\par
# Create the random grid\par
\par
random_grid = \{'n_estimators': n_estimators,\par
               'max_features': max_features,\par
               'max_depth': max_depth,\par
               'min_samples_split': min_samples_split,\par
               'min_samples_leaf': min_samples_leaf\}\par
# Random search of parameters, using 5 fold cross validation, \par
# search across 100 different combinations\par
rf_random = RandomizedSearchCV(estimator = reg_rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)\par
rf_random.fit(X_train,y_train)\par
Fitting 5 folds for each of 10 candidates, totalling 50 fits\par
[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=5, min_samples_split=5, n_estimators=900; total time=   5.6s\par
[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=5, min_samples_split=5, n_estimators=900; total time=   5.7s\par
[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=5, min_samples_split=5, n_estimators=900; total time=   5.2s\par
[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=5, min_samples_split=5, n_estimators=900; total time=   4.9s\par
[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=5, min_samples_split=5, n_estimators=900; total time=   5.2s\par
[CV] END max_depth=15, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=1100; total time=   7.9s\par
[CV] END max_depth=15, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=1100; total time=   7.9s\par
[CV] END max_depth=15, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=1100; total time=   7.9s\par
[CV] END max_depth=15, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=1100; total time=   8.7s\par
[CV] END max_depth=15, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=1100; total time=   8.6s\par
C:\\Users\\lodhi\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\par
  warn(\par
[CV] END max_depth=15, max_features=auto, min_samples_leaf=5, min_samples_split=100, n_estimators=300; total time=   5.0s\par
C:\\Users\\lodhi\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\par
  warn(\par
[CV] END max_depth=15, max_features=auto, min_samples_leaf=5, min_samples_split=100, n_estimators=300; total time=   4.9s\par
C:\\Users\\lodhi\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\par
  warn(\par
[CV] END max_depth=15, max_features=auto, min_samples_leaf=5, min_samples_split=100, n_estimators=300; total time=   4.7s\par
C:\\Users\\lodhi\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\par
  warn(\par
[CV] END max_depth=15, max_features=auto, min_samples_leaf=5, min_samples_split=100, n_estimators=300; total time=   4.9s\par
C:\\Users\\lodhi\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\par
  warn(\par
[CV] END max_depth=15, max_features=auto, min_samples_leaf=5, min_samples_split=100, n_estimators=300; total time=   5.1s\par
C:\\Users\\lodhi\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\par
  warn(\par
[CV] END max_depth=15, max_features=auto, min_samples_leaf=5, min_samples_split=5, n_estimators=400; total time=   8.9s\par
C:\\Users\\lodhi\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\par
  warn(\par
[CV] END max_depth=15, max_features=auto, min_samples_leaf=5, min_samples_split=5, n_estimators=400; total time=   8.6s\par
C:\\Users\\lodhi\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\par
  warn(\par
[CV] END max_depth=15, max_features=auto, min_samples_leaf=5, min_samples_split=5, n_estimators=400; total time=   8.2s\par
C:\\Users\\lodhi\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\par
  warn(\par
[CV] END max_depth=15, max_features=auto, min_samples_leaf=5, min_samples_split=5, n_estimators=400; total time=   8.1s\par
C:\\Users\\lodhi\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\par
  warn(\par
[CV] END max_depth=15, max_features=auto, min_samples_leaf=5, min_samples_split=5, n_estimators=400; total time=   7.3s\par
C:\\Users\\lodhi\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\par
  warn(\par
[CV] END max_depth=20, max_features=auto, min_samples_leaf=10, min_samples_split=5, n_estimators=700; total time=  11.6s\par
C:\\Users\\lodhi\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\par
  warn(\par
[CV] END max_depth=20, max_features=auto, min_samples_leaf=10, min_samples_split=5, n_estimators=700; total time=  11.3s\par
C:\\Users\\lodhi\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\par
  warn(\par
[CV] END max_depth=20, max_features=auto, min_samples_leaf=10, min_samples_split=5, n_estimators=700; total time=  11.7s\par
C:\\Users\\lodhi\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\par
  warn(\par
[CV] END max_depth=20, max_features=auto, min_samples_leaf=10, min_samples_split=5, n_estimators=700; total time=  11.7s\par
C:\\Users\\lodhi\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\par
  warn(\par
[CV] END max_depth=20, max_features=auto, min_samples_leaf=10, min_samples_split=5, n_estimators=700; total time=  11.1s\par
[CV] END max_depth=25, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=  12.2s\par
[CV] END max_depth=25, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=  10.8s\par
[CV] END max_depth=25, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=  10.7s\par
[CV] END max_depth=25, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=  10.4s\par
[CV] END max_depth=25, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=  10.2s\par
[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=10, min_samples_split=15, n_estimators=1100; total time=   4.0s\par
[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=10, min_samples_split=15, n_estimators=1100; total time=   3.7s\par
[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=10, min_samples_split=15, n_estimators=1100; total time=   3.8s\par
[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=10, min_samples_split=15, n_estimators=1100; total time=   3.8s\par
[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=10, min_samples_split=15, n_estimators=1100; total time=   3.4s\par
[CV] END max_depth=15, max_features=sqrt, min_samples_leaf=1, min_samples_split=15, n_estimators=300; total time=   1.7s\par
[CV] END max_depth=15, max_features=sqrt, min_samples_leaf=1, min_samples_split=15, n_estimators=300; total time=   1.6s\par
[CV] END max_depth=15, max_features=sqrt, min_samples_leaf=1, min_samples_split=15, n_estimators=300; total time=   1.7s\par
[CV] END max_depth=15, max_features=sqrt, min_samples_leaf=1, min_samples_split=15, n_estimators=300; total time=   1.8s\par
[CV] END max_depth=15, max_features=sqrt, min_samples_leaf=1, min_samples_split=15, n_estimators=300; total time=   1.7s\par
[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=700; total time=   2.5s\par
[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=700; total time=   2.4s\par
[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=700; total time=   2.4s\par
[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=700; total time=   2.4s\par
[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=700; total time=   2.4s\par
C:\\Users\\lodhi\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\par
  warn(\par
[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=15, n_estimators=700; total time=  13.9s\par
C:\\Users\\lodhi\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\par
  warn(\par
[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=15, n_estimators=700; total time=  14.4s\par
C:\\Users\\lodhi\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\par
  warn(\par
[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=15, n_estimators=700; total time=  12.6s\par
C:\\Users\\lodhi\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\par
  warn(\par
[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=15, n_estimators=700; total time=  13.9s\par
C:\\Users\\lodhi\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\par
  warn(\par
[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=15, n_estimators=700; total time=  14.0s\par
C:\\Users\\lodhi\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\par
  warn(\par
RandomizedSearchCV\par
estimator: RandomForestRegressor\par
\par
RandomForestRegressor\par
rf_random.best_params_\par
\{'n_estimators': 700,\par
 'min_samples_split': 15,\par
 'min_samples_leaf': 1,\par
 'max_features': 'auto',\par
 'max_depth': 20\}\par
prediction = rf_random.predict(X_test)\par
plt.figure(figsize = (8,8))\par
sns.distplot(y_test-prediction)\par
plt.show()\par
C:\\Users\\lodhi\\anaconda3\\lib\\site-packages\\seaborn\\distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\par
  warnings.warn(msg, FutureWarning)\par
\par
plt.figure(figsize = (8,8))\par
plt.scatter(y_test, prediction, alpha = 0.5)\par
plt.xlabel("y_test")\par
plt.ylabel("y_pred")\par
plt.show()\par
\par
print('MAE:', metrics.mean_absolute_error(y_test, prediction))\par
print('MSE:', metrics.mean_squared_error(y_test, prediction))\par
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, prediction)))\par
MAE: 1166.0782896719822\par
MSE: 4058012.941124271\par
RMSE: 2014.451027234038\par
# Save the model to reuse it again\par
import pickle\par
# open a file, where you ant to store the data\par
file = open('flight_rf.pkl', 'wb')\par
\par
# dump information to that file\par
pickle.dump(rf_random, file)\par
model = open('flight_rf.pkl','rb')\par
forest = pickle.load(model)\par
y_prediction = forest.predict(X_test)\par
metrics.r2_score(y_test, y_prediction)\par
0.8117984258960966\lang9\par
}
 